Open source framework to build apps powered by LLMs

semantic search:
 - split the whole text into chunks (text splitter)
 - embed context text to vectors, and embed the query too
 - store them in VetorDB
 - find the similarity of query vector with text vectors to give the relevant context

langchain benefits
 - chain concept, pipeline is easier to build
 - model agnostic development (can change components easily, not the whole code )
 - complete ecosystem (every component has high variety)
 - in convo memory and state is maintained

Alternatives:
 - Llamaindex
 - Haystack 

Components:
  - Models: 
      - should have NLU and context aware text-gen
      - trained on the whole internet, and billions of params
      - LLms stored on their server and public uses APIs so that everyone can use
      - Langchain gives standardization as we can use any LLM api
      - there are 2 models in langchain(LLms(text as input, text as output), embedding(text as input, vectors as output))

  - Prompts
      - input to LLMs
      - very sensitive to these instructions
      - with langchain we can create dynamic prompts, role based prompts(user, system)
      - few shot prompts - give examples to the LLM to explain it how to give answer

  - Chains
      - build pipelines in Langchain
      - build stages and the output of stage is input of the other
      - can build quite complex pipelines, parallel, conditional, etc
  
  -Indexes~
      - connect app to external KBs. 
      - Doc Loader, Text Splitter, VectorDBs, retrievers

  - Memory
      - LLM api calls are stateless
      - Langchain stores convos to keep context
      - BufferMemory, BufferWindowMemory, SummarizerBasedMemory, CustomMemory

  - Agents
      - can create AI agents easily here

** Models **

- Interface to interact with different models
- 2 types: Language (LLMs, Chat models) and Embedding Models
      -LLM: text input, text output
      -Embedding: text input, vector output

- LLMs are general purpose: text generation, summarization, etc. Support for these is getting obsolete

- Chat models: Used for conversational tasks. Main foucs. Fine tuned on huge data and fine-tuned on chat datasets, convo history is maintained, assign roles to the models.  

      -Temperature: Controls the randomness of a model, affecting the creativness of the model. ranges from 0 - 1.5+.
      0 is deterministic. Higher is random nd creative.
- open source models can be downloaded and then easily used on confidential docs, can be downloaded from huggingface
      - also HF inference api can be used (free to some limit.)
      - high hardware requirements
      - setup complexity when downloaded
      - not fine tuned enough
      - limited mulitmodal capabilities

StructuredOutput - models returning json format rather than free text.
    - eg. data extraction, API building, Agents
    - langchain has with_structured_output and tell the data_fromat functionality for some LLMs which support this.
    - json_schema is specified by 3 ways: TypedDict, Pydantic, json_schema
        - TypedDict: define a dict in python and it follows a specific structure defined beforehand, but even if we specifiy the datatype in the key, we can still change it afterwards, there is no rule. {from typing import TypedDict}
        - Pydantic: It is a data validation and data parsing lib. It sures the json is type safe. We can have optional fields, Emailstr verification, constraints application
        -json_schema: when we are working with multiple languages, as json is unoiversal, it is quite beneficial


Output Parsers - convert raw LLM outputs into structured output formats like json, csv, pydantic models. they ensure consistency, validation in apps
    -strOutputParser: simplest one, returns plain string
    -jsonOutputpasrser: forces LLM to give ans in json, but u cannot enforce a schema, only through instructions to LLMs not via code.
    -StructuredOutputParser: extract structured JSON data from LLM based on a predefined field schema, but data validation is not possible. 
    -PydanticOutputParser

Chains: Multiple steps are involved, basic is Prompt --> LLM --> Output
    - one way is to manually do it
    - using chains we can create pipelines quite easily
    - we can make sequential, parallel, conditional chains

Runnables: We need to know runnables to understand chains
    - every llm app has a prompt and sending to LLM, and devs used to do this manually, so langchain made a chains to do this automatically in a single function call. So for different complex redundant tasks across apps they created chains for them.
    - But they created too many chains for each usecases, coz of that their codebase got bigger with different keywords, and learning curve for AI enggs got steep.
    - So Runnables are like unit of work, give them input, they process and give output
        - each runnable follows same methods eg. invoke, batch, stream
        - easily connect runnables and it becomes a bigger runnable
        - they are like LEGO blocks 

        - There are 2 types of runnables:
            - task specific - langchain comps which got converted to runnables to be used in pipelines, eg. PromptTemplate, Retriever, etc
            - Runnable Primitive: fundamental building blocks defining how different runnables interact.

    -RunnableSequence: sequential chain of runnables passing output of one to another as input
    - RunnableParallel: Parallel chains, each runnable receives same input, processes it independently, producing a dict of outputs
    - RunnablePassthrough: simply return the input as output,
    needed when we want to get the previous output also
    - RunnableLambda: apply custom functions within AI pipelines
    - RunnableBranch: a control flow comp, conditionally route input data to diff chains or runnables on custom logic
    -LCEL (Langchain expression language): runnableSequence is used everywhere, so they decided to make chains like x1 | x2 | x3