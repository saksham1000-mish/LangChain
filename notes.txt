Open source framework to build apps powered by LLMs

semantic search:
 - split the whole text into chunks (text splitter)
 - embed context text to vectors, and embed the query too
 - store them in VetorDB
 - find the similarity of query vector with text vectors to give the relevant context

langchain benefits
 - chain concept, pipeline is easier to build
 - model agnostic development (can change components easily, not the whole code )
 - complete ecosystem (every component has high variety)
 - in convo memory and state is maintained

Alternatives:
 - Llamaindex
 - Haystack 

Components:
  - Models: 
      - should have NLU and context aware text-gen
      - trained on the whole internet, and billions of params
      - LLms stored on their server and public uses APIs so that everyone can use
      - Langchain gives standardization as we can use any LLM api
      - there are 2 models in langchain(LLms(text as input, text as output), embedding(text as input, vectors as output))

  - Prompts
      - input to LLMs
      - very sensitive to these instructions
      - with langchain we can create dynamic prompts, role based prompts(user, system)
      - few shot prompts - give examples to the LLM to explain it how to give answer

  - Chains
      - build pipelines in Langchain
      - build stages and the output of stage is input of the other
      - can build quite complex pipelines, parallel, conditional, etc
  
  -Indexes~
      - connect app to external KBs. 
      - Doc Loader, Text Splitter, VectorDBs, retrievers

  - Memory
      - LLM api calls are stateless
      - Langchain stores convos to keep context
      - BufferMemory, BufferWindowMemory, SummarizerBasedMemory, CustomMemory

  - Agents
      - can create AI agents easily here

** Models **

- Interface to interact with different models
- 2 types: Language (LLMs, Chat models) and Embedding Models
      -LLM: text input, text output
      -Embedding: text input, vector output

- LLMs are general purpose: text generation, summarization, etc. Support for these is getting obsolete

- Chat models: Used for conversational tasks. Main foucs. Fine tuned on huge data and fine-tuned on chat datasets, convo history is maintained, assign roles to the models.  

      -Temperature: Controls the randomness of a model, affecting the creativness of the model. ranges from 0 - 1.5+.
      0 is deterministic. Higher is random nd creative.
- open source models can be downloaded and then easily used on confidential docs, can be downloaded from huggingface
      - also HF inference api can be used (free to some limit.)
      - high hardware requirements
      - setup complexity when downloaded
      - not fine tuned enough
      - limited mulitmodal capabilities

StructuredOutput - models returning json format rather than free text.
    - eg. data extraction, API building, Agents
    - langchain has with_structured_output and tell the data_fromat functionality for some LLMs which support this.
    - json_schema is specified by 3 ways: TypedDict, Pydantic, json_schema
        - TypedDict: define a dict in python and it follows a specific structure defined beforehand, but even if we specifiy the datatype in the key, we can still change it afterwards, there is no rule. {from typing import TypedDict}
        - Pydantic: It is a data validation and data parsing lib. It sures the json is type safe. We can have optional fields, Emailstr verification, constraints application
        -json_schema: when we are working with multiple languages, as json is unoiversal, it is quite beneficial


Output Parsers - convert raw LLM outputs into structured output formats like json, csv, pydantic models. they ensure consistency, validation in apps
    -strOutputParser: simplest one, returns plain string
    -jsonOutputpasrser: forces LLM to give ans in json, but u cannot enforce a schema, only through instructions to LLMs not via code.
    -StructuredOutputParser: extract structured JSON data from LLM based on a predefined field schema, but data validation is not possible. 
    -PydanticOutputParser

Chains: Multiple steps are involved, basic is Prompt --> LLM --> Output
    - one way is to manually do it
    - using chains we can create pipelines quite easily
    - we can make sequential, parallel, conditional chains

Runnables: We need to know runnables to understand chains
    - every llm app has a prompt and sending to LLM, and devs used to do this manually, so langchain made a chains to do this automatically in a single function call. So for different complex redundant tasks across apps they created chains for them.
    - But they created too many chains for each usecases, coz of that their codebase got bigger with different keywords, and learning curve for AI enggs got steep.
    - So Runnables are like unit of work, give them input, they process and give output
        - each runnable follows same methods eg. invoke, batch, stream
        - easily connect runnables and it becomes a bigger runnable
        - they are like LEGO blocks 

        - There are 2 types of runnables:
            - task specific - langchain comps which got converted to runnables to be used in pipelines, eg. PromptTemplate, Retriever, etc
            - Runnable Primitive: fundamental building blocks defining how different runnables interact.

    -RunnableSequence: sequential chain of runnables passing output of one to another as input
    - RunnableParallel: Parallel chains, each runnable receives same input, processes it independently, producing a dict of outputs
    - RunnablePassthrough: simply return the input as output,
    needed when we want to get the previous output also
    - RunnableLambda: apply custom functions within AI pipelines
    - RunnableBranch: a control flow comp, conditionally route input data to diff chains or runnables on custom logic
    -LCEL (Langchain expression language): runnableSequence is used everywhere, so they decided to make chains like x1 | x2 | x3

RAG: combines info retrieval with language gen, where model retrieves relevant docs from a knowledge base, and uses them as context.

    DocumentLoaders: used to load data from various sources into a standardized form, which can be used for chunking, embediing, retrieval and generation. loads doc as a list of docs
    - Document(
        page_conetent="...",
        metadata={"source":"...", ...}
    )
    - There are many doc loaders, most used are: TextLoader, PyPDFLoader, WebBaseLoader, CSVLoader
        -TextLoader: reads plain text in .txt files
        -PyPDFLoader: load content from PDF files and convert each page into a document object. so if 20 pg doc, converts to 20 doc objects. Not great with scannedPDFs or complex layouts. doesnt work on scannedPDFs that well. There are many different types of PDF loaders
        -DirectoryLoader - loads multiple docs from a directory
        -WebPageLoader: extract and load text from webpages using BeautifulSoup to parse HTML and extract visible text. It struggles with JS heavy pages we can use SeleniumURLLoader for that. It loads only Static content (what's in the HTML structure)
        -CSVLoader: load CSV files to Document objects

    LazyLoad: loads on demand, but normal load does everything at once
        - This returns a generator of Document objects, as oppose to a list of Document objects
        - Used when there are huge number of files and u want to do stream processing, without using lots of memory